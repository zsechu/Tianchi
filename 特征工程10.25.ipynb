{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "print(sys.path)\n",
    "from tool import tool\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data2/数据预处理10.25.csv',parse_dates=['date'])\n",
    "# data = data[~(data.Temp == 0.0)].reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_rolling_columns = ['Spd','Temp']\n",
    "fe_lag_columns = ['Spd','Temp']\n",
    "fe_diff_columns = ['Spd','Temp']\n",
    "fe_div_columns = ['Temp']\n",
    "# fe_groupby_columns = ['Spd','Temp']\n",
    "# fe_max_min_columns = [\"Spd\",\"Temp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# temp + spd:  max-min in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fe_max_min(data, time_col,time_varying_cols):\n",
    "#     \"\"\"\n",
    "#     构造最大最小值\n",
    "#     :param df: DataFrame\n",
    "#     :param time_col: time column\n",
    "#     :param time_varying_cols: time varying columns\n",
    "#     :param lag: lag list\n",
    "#     :return: DataFrame lag\n",
    "#     \"\"\"\n",
    "#     df = data.copy()\n",
    "#     result = pd.DataFrame({time_col:df[time_col].values},index=df.index)\n",
    "#     add_fetures = []\n",
    "#     for column in time_varying_cols:\n",
    "#         name = f'{column}_max_min'\n",
    "#         add_fetures.append(name)\n",
    "#         df[name] = df.groupby([\"Day\"])[column].transform(lambda x: (x.max() - x.min()))\n",
    "#     return result.merge(df[[time_col,] + add_fetures], on=time_col, how='left')[add_fetures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"max-min\"] = data.groupby([\"Day\"])[\"Temp\"].transform(lambda x: (x.max() - x.min()))\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_max_min = fe_max_min(data,\"date\",fe_max_min_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rolling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolling = tool.fe_rolling_stat(data, time_col='date', time_varying_cols=fe_rolling_columns, window_size=[3,6,24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lag = tool.fe_lag(data, 'date', fe_lag_columns, [1,2,3,24,48,72])  \n",
    "df_lag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一阶差分和差除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* diff：不同lag步长的一阶差分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_diff(data, time_col,time_varying_cols):\n",
    "    \"\"\"\n",
    "    构造差分特征:这里和diff函数的效果不一样(diff: lag0 - lag1,lag0 - lag2, lag0 - lag3)\n",
    "    lag0 - lag1 \n",
    "    lag1 - lag2\n",
    "    lag2 - lag3\n",
    "    :param df: dataFrame\n",
    "    :param time_col: time column\n",
    "    :param time_varying_cols: time varying columns\n",
    "    :param lag: lag list\n",
    "    :return: dataFrame lag\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    result = pd.DataFrame({time_col:df[time_col].values},index=df.index)\n",
    "    add_fetures = []\n",
    "    for column in time_varying_cols:\n",
    "        name = f'{column}_onehour_diff'          # 这个名字根据情况自己取\n",
    "        add_fetures.append(name)\n",
    "        df[name] = df[column] - df[column].shift(1)\n",
    "\n",
    "        name = f'{column}_twohour_diff'  # 这个名字根据情况自己取\n",
    "        add_fetures.append(name)\n",
    "        df[name] = df[column].shift(1) - df[column].shift(2)\n",
    " \n",
    "        name = f'{column}_threehour_diff'   # 这个名字根据情况自己取\n",
    "        add_fetures.append(name)\n",
    "        df[name] = df[column].shift(2) - df[column].shift(3)    # lag2 - lag3\n",
    "        \n",
    "    \n",
    "    return result.merge(df[[time_col,] + add_fetures], on=time_col, how='left')[add_fetures]\n",
    "\n",
    "def fe_div(data, time_col,time_varying_cols):    \n",
    "    \"\"\"\n",
    "    构造一阶差分特征,使用不同的lag\n",
    "    注意不要有0值\n",
    "    :param df: dataFrame\n",
    "    :param time_col: time column\n",
    "    :param time_varying_cols: time varying columns\n",
    "    :param lag: lag list\n",
    "    :return: dataFrame lag\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    result = pd.DataFrame({time_col:df[time_col].values},index=df.index)\n",
    "    add_fetures = []\n",
    "    for column in time_varying_cols:\n",
    "        name = f'{column}_onehour_div'\n",
    "        add_fetures.append(name)\n",
    "        df[name] = np.divide(df[column], df[column].shift(1))\n",
    "\n",
    "        name = f'{column}_twohour_div'\n",
    "        add_fetures.append(name)\n",
    "        df[name] = np.divide(df[column].shift(1), df[column].shift(2))\n",
    "\n",
    "        name = f'{column}_threehour_div'\n",
    "        add_fetures.append(name)\n",
    "        df[name] = np.divide(df[column].shift(2), df[column].shift(3))\n",
    "    \n",
    "    \n",
    "    return  result.merge(df[[time_col,] + add_fetures], on=time_col, how='left')[add_fetures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = fe_diff(data, 'date', fe_diff_columns)\n",
    "df_div = fe_div(data, 'date', fe_div_columns)\n",
    "df_diff.shape, df_div.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根据一阶构造二阶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意这里新增了date列，后面合并的时候记得删除\n",
    "df_diff[\"date\"] = data[\"date\"]\n",
    "df_div[\"date\"] = data[\"date\"]\n",
    "fe_diff_2jie_columns = [x for x in list(df_diff.columns) if x != \"date\"]\n",
    "fe_div_2jie_columns = [x for x in list(df_div.columns) if x != \"date\"]\n",
    "\n",
    "# 2阶差分差除\n",
    "df_diff_2jie = tool.fe_diff(df_diff, 'date', fe_diff_2jie_columns,[1])  # lag0 - lag1\n",
    "df_div_2jie = tool.fe_div(df_div, 'date', fe_div_2jie_columns,[1])\n",
    "df_diff_2jie.shape,df_div_2jie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_diff1 = tool.fe_diff(data, 'date', fe_diff_columns,[1,2,3])  \n",
    "# df_div1 = tool.fe_div(data, 'date', fe_div_columns,[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练集\n",
    "- 注意：避免数据泄露，训练集、测试集分开构造特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_days = 10\n",
    "# delta = timedelta(days=valid_days)\n",
    "# time_col = 'date'\n",
    "# test_time_split = data[time_col].max() - delta\n",
    "# test_idx = data.loc[data[time_col] > test_time_split].index\n",
    "# train_idx = data.loc[data[time_col] <= test_time_split].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_idx.shape,train_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 增加月份信息\n",
    "# data['Month'] = data[time_col].dt.month\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 一年内的每个月的temp、spd同比的统计值\n",
    "# df_groupby_train = tool.fe_groupby(data.loc[train_idx], 'date', ['Hour','Month'], fe_diff_columns)\n",
    "# df_groupby_test = tool.fe_groupby(data.loc[test_idx], 'date', ['Hour','Month'], fe_diff_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_groupby = pd.concat([df_groupby_train,df_groupby_test],axis=0).reset_index(drop=True)\n",
    "# df_groupby.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fe_groupby(data, time_col, by, time_varying_cols):\n",
    "    '''\n",
    "    一年内的temp、spd同比的统计值(不是一个月)\n",
    "    '''\n",
    "    df = data.copy()\n",
    "    result = pd.DataFrame({time_col:df[time_col].values},index=df.index)\n",
    "    add_fetures = []\n",
    "    for val in time_varying_cols:\n",
    "        for op in ['mean','std','median','max','min','skew']:\n",
    "            name = f'{val}__{by[0]}__{op}'\n",
    "            add_fetures.append(name)\n",
    "            df[name] = df.groupby(by)[val].transform(op)\n",
    "    \n",
    "    return result.merge(df[[time_col,] + add_fetures], on=time_col, how='left')[add_fetures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_groupby_train = fe_groupby(data.loc[train_idx], 'date', ['Hour'], fe_diff_columns)\n",
    "# df_groupby_test = fe_groupby(data.loc[test_idx], 'date', ['Hour'], fe_diff_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_groupby_2 = pd.concat([df_groupby_train,df_groupby_test],axis=0).reset_index(drop=True)\n",
    "# df_groupby_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_entropy = pd.concat([data[[\"date\",\"Day\",\"Temp\",\"Spd\"]],df_lag_R],axis=1)\n",
    "# data_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fe_qbins(data,time_col,use_cols,n_bins):\n",
    "#     '''\n",
    "#     :param data: dataFrame\n",
    "#     :param time_col: time column\n",
    "#     :param use_cols: list\n",
    "#     :param nbins: int\n",
    "#     '''\n",
    "#     df = data.copy()\n",
    "#     result = pd.DataFrame({time_col:df[time_col].values},index=df.index)\n",
    "#     add_features = []\n",
    "#     for col in use_cols:\n",
    "#         name = f\"{col}_{n_bins}\"\n",
    "#         add_features.append(name)\n",
    "#         df[name] = pd.qcut(df[col],n_bins)\n",
    "#     return result.merge(df[[time_col,] + add_features], on=time_col, how='left')[add_features]\n",
    "\n",
    "# def fe_bins(data,time_col,use_cols,n_bins):\n",
    "#     '''\n",
    "#     :param data: dataFrame\n",
    "#     :param time_col: time column\n",
    "#     :param use_cols: list\n",
    "#     :param nbins: int\n",
    "#     '''\n",
    "#     df = data.copy()\n",
    "#     result = pd.DataFrame({time_col:df[time_col].values},index=df.index)\n",
    "#     add_features = []\n",
    "#     for col in use_cols:\n",
    "#         name = f\"{col}_{n_bins}\"\n",
    "#         add_features.append(name)\n",
    "#         df[name] = pd.cut(df[col],n_bins)\n",
    "#     return result.merge(df[[time_col,] + add_features], on=time_col, how='left')[add_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bins1 = fe_qbins(data_entropy,\"date\",[\"Temp\",\"Spd\"],10)  # 等距分箱\n",
    "# df_bins2 = fe_bins(data_entropy,\"date\",[\"Radiation_lag_240\"],20)  # 等频分箱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OrdinalEncoder\n",
    "# ordinalencoder = OrdinalEncoder()\n",
    "\n",
    "# for col in df_bins1.columns:\n",
    "#     df_bins1[col] = ordinalencoder.fit_transform(df_bins1[col].values.reshape(-1,1)) \n",
    "# for col in df_bins2.columns:\n",
    "#     df_bins2[col] = ordinalencoder.fit_transform(df_bins2[col].values.reshape(-1,1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_entropy = pd.concat([data_entropy,df_bins1,df_bins2],axis=1)\n",
    "# data_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fes_entropy_cols = [\"Temp_10\",\"Spd_10\",\"Radiation_lag_240_20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fe_entropy(data,time_col,use_cols):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    result = pd.DataFrame({time_col:df[time_col].values},index=df.index)\n",
    "    add_fetures = []\n",
    "    for col in use_cols:\n",
    "        name = f'{col}_entropy'\n",
    "        add_fetures.append(name)\n",
    "        S = -(df.groupby([\"Day\"])[col].value_counts()/24) * np.log(df.groupby([\"Day\"])[col].value_counts()/24)\n",
    "        S.index.names = [\"Day\",f\"{col}_value\"]\n",
    "        S = S.reset_index()\n",
    "        S.columns = [\"Day\",f'{col}',name]\n",
    "        df_tmp = S.groupby([\"Day\"])[name].sum()\n",
    "        df = pd.merge(df,df_tmp,on=\"Day\",how=\"left\")\n",
    "    return result.merge(df[[time_col] + add_fetures], on=time_col, how='left')[add_fetures]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_entropy = fe_entropy(data_entropy,\"date\",use_cols = fes_entropy_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并Temp+Spd的熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 两者的大小不一样，先归一化后再合并\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# trans_df_entropy = [\"Temp_10_entropy\",\"Spd_10_entropy\"]\n",
    "# for fea in trans_df_entropy:\n",
    "#     df_entropy[fea] = scaler.fit_transform(df_entropy[fea].values.reshape(-1,1))\n",
    "\n",
    "# df_entropy[\"Temp_Spd_entropy\"] = df_entropy['Temp_10_entropy'] * 0.5 + df_entropy['Spd_10_entropy'] * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = tool.feature_combination([data,df_rolling, df_lag,df_lag_R,df_diff.drop(\"date\",axis=1),df_div.drop(\"date\",axis=1),\\\n",
    "#     df_diff_2jie,df_div_2jie,df_groupby,df_groupby_2,df_entropy,df_max_min])\n",
    "\n",
    "df_all = tool.feature_combination([data,df_rolling, df_lag,df_diff.drop(\"date\",axis=1),df_div.drop(\"date\",axis=1),\\\n",
    "    df_diff_2jie,df_div_2jie])\n",
    "\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1012.to_csv('./new_fes_1012_2.0.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对rooling特征继续构造差分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_rolling_diff_columns = ['Spd__3h__mean','Temp__3h__mean','Spd__6h__mean','Temp__6h__mean','Spd__24h__mean','Temp__24h__mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rolling_diff = fe_diff(df_all, \"date\", fe_rolling_diff_columns)\n",
    "df_rolling_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = tool.feature_combination([df_all, df_rolling_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正余弦编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours_in_day = 24\n",
    "# hour\n",
    "df_all['sin_hour'] = np.sin(2 * np.pi * df_all[\"Hour\"]/hours_in_day)\n",
    "df_all['cos_hour'] = np.cos(2 * np.pi * df_all[\"Hour\"]/hours_in_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.to_csv(\"./data2/特征工程10.25.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对熵特征构造一阶差分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe_entropy_diff_cols = [\"Temp_10_entropy\",\"Spd_10_entropy\",\"Temp_Spd_entropy\"]\n",
    "# df_entropy_diff =  tool.fe_diff(df_all,\"date\",fe_entropy_diff_cols, [1])\n",
    "# df_all = tool.feature_combination([df_all, df_entropy_diff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_entropy_diff.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all['Radiation'].fillna(0.0,inplace=True)\n",
    "# df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_added_feas = [\"date\",'Temp_10_entropy_diff_1', 'Spd_10_entropy_diff_1','Temp_Spd_entropy_diff_1',\"Spd_max_min\",\"Temp_max_min\"]\n",
    "# df_all[new_added_feas].to_csv('./new_feas_1017.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5bcff1009bd0b8d5d29c38b55165c97510dc4ebe32fbd78e3def37853c7c76a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
